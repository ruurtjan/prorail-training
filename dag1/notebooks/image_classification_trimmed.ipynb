{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on data engineering Vantage AI\n",
    "\n",
    "Deze sessie gaan we neurale netwerken trainen om simpele images te classificeren van de CIFAR-10 dataset. \n",
    "\n",
    "## Dependency management\n",
    "Deze notebook gaat er vanuit dat je de volgende python dependencies geïnstalleerd hebt:\n",
    "- Jupyter\n",
    "- Tensorflow\n",
    "- Keras\n",
    "- Matplotlib\n",
    "- SKLearn\n",
    "\n",
    "Opdracht: _Schrijf een `requirements.txt` waarmee de requirements van deze notebook makkelijk geïnstalleerd kunnen worden._\n",
    "\n",
    "## Data inladen\n",
    "\n",
    "De data bestaat uit 3 delen: train, validatie en test set.\n",
    "\n",
    "Opdracht: _Er is veel herhaling in deze code. Splits dit op in leesbare en herbruikbare code. Denk hierbij aan de engineering principes die we hebben behandeld._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # training set, batches 1-4\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), \"data\")):\n",
    "        os.makedirs(os.path.join(os.getcwd(), \"data\"))\n",
    "\n",
    "        \n",
    "    dataset_dir = os.path.join(os.getcwd(), \"data\")\n",
    "    \n",
    "    if not os.path.exists(os.path.join(dataset_dir, \"cifar-10-batches-py\")):\n",
    "        print(\"Downloading data...\")\n",
    "        urlretrieve(\"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", os.path.join(dataset_dir, \"cifar-10-python.tar.gz\"))\n",
    "        tar = tarfile.open(os.path.join(dataset_dir, \"cifar-10-python.tar.gz\"))\n",
    "        tar.extractall(dataset_dir)\n",
    "        tar.close()\n",
    "        \n",
    "    train_X = np.zeros((40000, 3, 32, 32), dtype=\"float32\")\n",
    "    train_y = np.zeros((40000, 1), dtype=\"ubyte\").flatten()\n",
    "    n_samples = 10000  # aantal samples per batch\n",
    "    dataset_dir = os.path.join(dataset_dir,\"cifar-10-batches-py\")\n",
    "    for i in range(0,4):\n",
    "        f = open(os.path.join(dataset_dir, \"data_batch_\"+str(i+1)), \"rb\")\n",
    "        cifar_batch = pickle.load(f,encoding=\"latin1\")\n",
    "        f.close()\n",
    "        train_X[i*n_samples:(i+1)*n_samples] = (cifar_batch['data'].reshape(-1, 3, 32, 32) / 255.).astype(\"float32\")\n",
    "        train_y[i*n_samples:(i+1)*n_samples] = np.array(cifar_batch['labels'], dtype='ubyte')\n",
    "\n",
    "    # validation set, batch 5\n",
    "    f = open(os.path.join(dataset_dir, \"data_batch_5\"), \"rb\")\n",
    "    cifar_batch_5 = pickle.load(f,encoding=\"latin1\")\n",
    "    f.close()\n",
    "    val_X = (cifar_batch_5['data'].reshape(-1, 3, 32, 32) / 255.).astype(\"float32\")\n",
    "    val_y = np.array(cifar_batch_5['labels'], dtype='ubyte')\n",
    "\n",
    "    # labels\n",
    "    f = open(os.path.join(dataset_dir, \"batches.meta\"), \"rb\")\n",
    "    cifar_dict = pickle.load(f,encoding=\"latin1\")\n",
    "    label_to_names = {k:v for k, v in zip(range(10), cifar_dict['label_names'])}\n",
    "    f.close()\n",
    "\n",
    "    # test set\n",
    "    f = open(os.path.join(dataset_dir, \"test_batch\"), \"rb\")\n",
    "    cifar_test = pickle.load(f,encoding=\"latin1\")\n",
    "    f.close()\n",
    "    test_X = (cifar_test['data'].reshape(-1, 3, 32, 32) / 255.).astype(\"float32\")\n",
    "    test_y = np.array(cifar_test['labels'], dtype='ubyte')\n",
    "\n",
    "\n",
    "    print(\"training set size: data = {}, labels = {}\".format(train_X.shape, train_y.shape))\n",
    "    print(\"validation set size: data = {}, labels = {}\".format(val_X.shape, val_y.shape))\n",
    "    \n",
    "    print(\"Test set size: data = \"+str(test_X.shape)+\", labels = \"+str(test_y.shape))\n",
    "\n",
    "    return train_X, train_y, val_X, val_y, test_X, test_y, label_to_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Preprocessing\n",
    "Bij CIFAR10 is er niet veel preprocessing nodig. Normalisatie van de data is vaak een goed idee, vantevoren berekenen we de gemiddelde pixelwaarde en bij het batchgewijs trainen normaliseren we de data aan de hand van die waarde. Het is een goed idee om deze mean in een pickle bestand op te slaan, en die dan in te laden bij het opstarten zodat voor predicten niet de hele dataset nodig is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nr_channels = 3\n",
    "image_size = 32\n",
    "nr_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "train_X, train_y, val_X, val_y, test_X, test_y, label_to_names = load_data()\n",
    "\n",
    "# Conv nets trainen duurt erg lang op CPU, dus we gebruiken maar een klein deel\n",
    "# van de data nu, als er tijd over is kan je proberen je netwerk op de volledige set te runnen\n",
    "train_X = train_X[:10000]\n",
    "train_y = train_y[:10000]\n",
    "\n",
    "def calc_mean_std(X):\n",
    "    mean = np.mean(X)\n",
    "    std = np.std(X)\n",
    "    return mean, std\n",
    "\n",
    "def normalize(data, mean, std):\n",
    "    return (data-mean)/std\n",
    "\n",
    "#De data van train_X is genoeg om de mean en std van de hele set nauwkeurig te benaderen\n",
    "mean,std = calc_mean_std(train_X)\n",
    "test_X = normalize(test_X,mean,std)\n",
    "val_X = normalize(val_X,mean,std)\n",
    "train_X = normalize(train_X ,mean,std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definieer model\n",
    "We gebruiken de volledige images om een convolutioneel neuraal netwerk te definieren en te trainen. Alhoewel de data science niet de focus heeft in deze cursus is het wel belangrijk om te begrijpen wat er gebeurt, dus schroom niet ook vragen te stellen over het model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D, Input\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def conv_net():\n",
    "    # We definieren de input van het netwerk als de shape van de input,\n",
    "    # minus de dimensie van het aantal plaatjes, uiteindelijk dus (3, 32, 32).\n",
    "    input = Input(shape=train_X.shape[1:])\n",
    "    \n",
    "    # Eerste convolutielaag\n",
    "    # Padding valid betekent dat we enkel volledige convoluties gebruiken, zonder padding\n",
    "    # Data format channels_first betekent dat de channels eerst komen, en dan pas de size van ons plaatje\n",
    "    # Dus (3, 32, 32) in plaats van (32, 32, 3)\n",
    "    conv = Conv2D(filters=16, kernel_size=(3,3), padding='valid',\n",
    "                  data_format='channels_first', activation='relu')(input)\n",
    "    \n",
    "    # Nog een convolutielaag, dit keer met stride=2 om de inputsize te verkleinen\n",
    "    conv = Conv2D(filters=32, kernel_size=(3,3), padding='valid',\n",
    "                  data_format='channels_first', activation='relu', strides=(2, 2))(conv)\n",
    "    \n",
    "    #Voeg een flatten laag toe, om te schakelen naar de dense layer\n",
    "    flatten = Flatten()(conv)\n",
    "   \n",
    "    # De softmax laag voor de probabilities \n",
    "    output_layer = Dense(units=nr_classes, activation='softmax')(flatten)\n",
    "    \n",
    "    model = Model(inputs=input, outputs=output_layer)\n",
    "    \n",
    "    # Het model moet nog gecompiled worden en loss+learning functie gespecificeerd worden\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = conv_net()\n",
    "\n",
    "model.fit(x=train_X, y=train_y, batch_size=50, epochs=10, validation_data=(val_X, val_y), verbose=2)\n",
    "predictions = np.array(model.predict(test_X, batch_size=100))\n",
    "test_y = np.array(test_y, dtype=np.int32)\n",
    "#Take the highest prediction\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "#Print resultaten\n",
    "print(\"Accuracy = {}\".format(np.sum(predictions == test_y) / float(len(predictions))))\n",
    "print(classification_report(test_y, predictions, target_names=list(label_to_names.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opdracht\n",
    "Wat we graag willen is deze notebook uitgewerkt in een package met het cookiecutter template. We willen dan graag een splitsing tussen het trainingsdeel en het scoring deel. \n",
    "\n",
    "Het trainingsdeel levert een model en eventuele metadata op (opgeslagen op disk).  \n",
    "Het scoringsdeel gebruikt het model om de testset te predicten.  \n",
    "Runnen met: `python <filename> 'scoring'` of `python <filename> 'training'`\n",
    "\n",
    "Gebruik goede error handling voor bijvoorbeeld het predicten zonder model, of een verkeerd argument meegeven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
