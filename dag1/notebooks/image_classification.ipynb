{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kennissessie Neurale Netwerken\n",
    "\n",
    "# Deel 1\n",
    "Deze sessie gaan we neurale netwerken trainen om simpele images te classificeren van de CIFAR-10 dataset. \n",
    "\n",
    "**Stap 1: data inladen**\n",
    "\n",
    "De data bestaat uit 3 delen, waaronder de standaard train en testset, maar ook een validatieset die tijdens het trainen van het netwerk al een idee geeft hoe het netwerk op ongeziene data gaat performen.\n",
    "\n",
    "Let op: onderstaande stuk code is totaal niet belangrijk om te begrijpen voor de rest van de notebook. Het zorgt er alleen voor dat de data gedownload wordt en in de juiste mapjes wordt gezet. Besteed niet teveel tijd aan het precies begrijpen wat er gebeurt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # training set, batches 1-4\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), \"data\")):\n",
    "        os.makedirs(os.path.join(os.getcwd(), \"data\"))\n",
    "\n",
    "        \n",
    "    dataset_dir = os.path.join(os.getcwd(), \"data\")\n",
    "    \n",
    "    if not os.path.exists(os.path.join(dataset_dir, \"cifar-10-batches-py\")):\n",
    "        print(\"Downloading data...\")\n",
    "        urlretrieve(\"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", os.path.join(dataset_dir, \"cifar-10-python.tar.gz\"))\n",
    "        tar = tarfile.open(os.path.join(dataset_dir, \"cifar-10-python.tar.gz\"))\n",
    "        tar.extractall(dataset_dir)\n",
    "        tar.close()\n",
    "        \n",
    "    train_X = np.zeros((40000, 3, 32, 32), dtype=\"float32\")\n",
    "    train_y = np.zeros((40000, 1), dtype=\"ubyte\").flatten()\n",
    "    n_samples = 10000  # aantal samples per batch\n",
    "    dataset_dir = os.path.join(dataset_dir,\"cifar-10-batches-py\")\n",
    "    for i in range(0,4):\n",
    "        f = open(os.path.join(dataset_dir, \"data_batch_\"+str(i+1)), \"rb\")\n",
    "        cifar_batch = pickle.load(f,encoding=\"latin1\")\n",
    "        f.close()\n",
    "        train_X[i*n_samples:(i+1)*n_samples] = (cifar_batch['data'].reshape(-1, 3, 32, 32) / 255.).astype(\"float32\")\n",
    "        train_y[i*n_samples:(i+1)*n_samples] = np.array(cifar_batch['labels'], dtype='ubyte')\n",
    "\n",
    "    # validation set, batch 5\n",
    "    f = open(os.path.join(dataset_dir, \"data_batch_5\"), \"rb\")\n",
    "    cifar_batch_5 = pickle.load(f,encoding=\"latin1\")\n",
    "    f.close()\n",
    "    val_X = (cifar_batch_5['data'].reshape(-1, 3, 32, 32) / 255.).astype(\"float32\")\n",
    "    val_y = np.array(cifar_batch_5['labels'], dtype='ubyte')\n",
    "\n",
    "    # labels\n",
    "    f = open(os.path.join(dataset_dir, \"batches.meta\"), \"rb\")\n",
    "    cifar_dict = pickle.load(f,encoding=\"latin1\")\n",
    "    label_to_names = {k:v for k, v in zip(range(10), cifar_dict['label_names'])}\n",
    "    f.close()\n",
    "\n",
    "    # test set\n",
    "    f = open(os.path.join(dataset_dir, \"test_batch\"), \"rb\")\n",
    "    cifar_test = pickle.load(f,encoding=\"latin1\")\n",
    "    f.close()\n",
    "    test_X = (cifar_test['data'].reshape(-1, 3, 32, 32) / 255.).astype(\"float32\")\n",
    "    test_y = np.array(cifar_test['labels'], dtype='ubyte')\n",
    "\n",
    "\n",
    "    print(\"training set size: data = {}, labels = {}\".format(train_X.shape, train_y.shape))\n",
    "    print(\"validation set size: data = {}, labels = {}\".format(val_X.shape, val_y.shape))\n",
    "    print(\"test set size: data = {}, labels = {}\".format(test_X.shape, test_y.shape))\n",
    "\n",
    "    return train_X, train_y, val_X, val_y, test_X, test_y, label_to_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Shuffle en reshape data\n",
    "Bij veel datasets zit de data gegroepeerd waardoor het goed is om de trainingset te shufflen voordat het als input voor het neurale netwerk gebruikt wordt. Bij CIFAR10 is dit niet essentieel, maar we doen het toch even. \n",
    "De data moet daarna reshaped worden naar (nr_images, nr_channels\\*image_size\\*image_size) om correcte input voor onze MLP te leveren. Als we met CNN's aan de gang gaan is dit niet meer nodig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: data = (40000, 3, 32, 32), labels = (40000,)\n",
      "validation set size: data = (10000, 3, 32, 32), labels = (10000,)\n",
      "test set size: data = (10000, 3, 32, 32), labels = (10000,)\n"
     ]
    }
   ],
   "source": [
    "nr_channels = 3\n",
    "image_size = 32\n",
    "nr_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "train_X, train_y, val_X, val_y, test_X, test_y, label_to_names = load_data()\n",
    "\n",
    "#Shuffle trainingset\n",
    "train_set = list(zip(train_X, train_y))\n",
    "random.shuffle(train_set)\n",
    "train_X, train_y = zip(*train_set)\n",
    "train_X = np.array(train_X).reshape(40000,nr_channels*image_size*image_size)\n",
    "train_y = np.array(train_y)\n",
    "val_X = val_X.reshape(10000,nr_channels*image_size*image_size)\n",
    "test_X = test_X.reshape(10000,nr_channels*image_size*image_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Preprocessing\n",
    "Bij CIFAR10 is er niet veel preprocessing nodig. Normalisatie van de data is vaak een goed idee, vantevoren berekenen we de gemiddelde pixelwaarde en bij het batchgewijs trainen normaliseren we de data aan de hand van die waarde. Bij grotere datasets die niet volledig in je geheugen passen moet je hier vaak wat slimmers toepassen zoals een volledige pass door je data waarbij je de mean en variance streaming berekend met bijv. met Welford's algorithm. Tegenwoordig wordt ook vaak geen standaardisatie meer toegepast maar gewoon een simpele division door de gemiddelde pixelwaarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_mean_std(X):\n",
    "    mean = np.mean(X)\n",
    "    std = np.std(X)\n",
    "    return mean, std\n",
    "\n",
    "def normalize(data, mean, std):\n",
    "    return (data-mean)/std\n",
    "\n",
    "#De data van train_X is genoeg om de mean en std van de hele set nauwkeurig te benaderen\n",
    "mean,std = calc_mean_std(train_X)\n",
    "#train_y = train_y.reshape((-1, 1))\n",
    "test_X = normalize(test_X,mean,std)\n",
    "val_X = normalize(val_X,mean,std)\n",
    "train_X = normalize(train_X ,mean,std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Network\n",
    "Het is nu tijd om ons netwerk te definieren. We beginnen met een heel simpel MLP, en zullen dit later uitbreiden tot een mooi CNN. We gebruiken hierbij de \"functional API\" van Keras, in plaats van de Sequential API. Met Sequential kun je namelijk alleen maar sequentiele modellen definieren, en geen vertakkingen. Met de functional API kun je alles wat je maar wil, en de functional API is ook wat logischer in gebruik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "\n",
    "\n",
    "def mlp():\n",
    "    # Bij de eerste layer moeten we altijd input dimensions meegeven omdat TensorFlow en Theano STATIC computation graphs maken\n",
    "    # Bij iets als PyTorch is dat niet nodig, omdat PyTorch DYNAMIC computation graphs maakt\n",
    "    input = Input(shape=(nr_channels*image_size*image_size,))\n",
    "    \n",
    "    # Eerste dense layer, met een ReLU activatiefunctie\n",
    "    input_layer = Dense(units=100, activation='relu')(input)\n",
    "    hidden_layer = Dense(units=100, activation='relu')(input_layer)\n",
    "    \n",
    "    # Output layer heeft een softmax activatiefunctie in plaats van een ReLU.\n",
    "    # Softmax zorgt ervoor dat alle outputs optellen tot 1 en exponentieel genormaliseerd worden\n",
    "    # Zie de wikipedia pagina van softmax voor meer info\n",
    "    output_layer = Dense(units=nr_classes, activation='softmax')(hidden_layer)\n",
    "    \n",
    "    model = Model(inputs=input, outputs=output_layer)\n",
    "    \n",
    "    # Het model moet nog gecompiled worden en loss+learning functie gespecificeerd worden\n",
    "    # Als loss function gebruiken we sparse categorical crossentropy. Dat is log loss met sparse coding.\n",
    "    # Sparse coding betekent hier dat we geen one-hot encoding hebben toegepast op de labels,\n",
    "    # maar dat het direct integer category labels zijn.\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def mlp_2():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Minibatches \n",
    "Om het trainen wat vlotter te laten lopen gebruiken trainen we het netwerk per batch in plaats van per plaatje. De gradients worden dan berekend per batch in plaats van de hele set, waardoor het significant sneller traint vanwege parallelisatie-optimalisaties. Ook wordt de gradient gemiddeld over meerdere samples, wat een noisy gradient voorkomt. Minibatch training kan standaard in Keras bij het fitten van het model.\n",
    "\n",
    "### Pipeline\n",
    "Alle puzzelstukjes staan nu op zijn plek, dus kunnen we via een pipeline alles aan elkaar gaan linken en het model trainen en evalueren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = mlp()\n",
    "\n",
    "# model.summary() geeft een prachtig overzicht van je netwerk, inclusief in welke laag hoeveel parameters zitten\n",
    "model.summary()\n",
    "\n",
    "model.fit(x=train_X, y=train_y, batch_size=50, epochs=10, validation_data=(val_X,val_y), verbose=2)\n",
    "predictions = np.array(model.predict(test_X, batch_size=100))\n",
    "test_y = np.array(test_y, dtype=np.int32)\n",
    "# Pak de hoogste prediction\n",
    "predictions = np.argmax(predictions,axis=1)\n",
    "\n",
    "# Print resultaten\n",
    "print(\"Accuracy = {}\".format(np.sum(predictions == test_y) / float(len(predictions))))\n",
    "print(classification_report(test_y, predictions, target_names=list(label_to_names.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht\n",
    "Ons netwerk is nu erg simpel en geeft matige resultaten. Er zijn veel manieren om het netwerk beter te maken. In deze opdracht gaan we daarmee aan de slag. Maak veel gebruik van de [Keras documentatie](https://keras.io/)!\n",
    "\n",
    "* **Extra Layers**  bijvoorbeeld het toevoegen van meer layers. Pas model mlp() aan of maak een nieuw model in mlp_2() die meer layers heeft en kijk of dit betere resultaten oplevert. Het is verstandig om niet te vaak je model te evalueren op de testset, maar puur te kijken naar de training en validatie error of accuracy. Anders is het gevaar dat het netwerk overfit op de testset.\n",
    "\n",
    "* **Meer Units** Wat ook goed werkt is in plaats van meer layers toe te voegen, de bestaande layers groter maken. Dit kan door het verhogen van het aantal units per layer, waardoor er meer informatie per layer beschikbaar is. Experimenteer hiermee.\n",
    "\n",
    "* **Weight Initialisatie** De weights van de layers worden nu geïnitialiseerd vanuit de normaalverdeling. Kijk bij de [documentatie](https://keras.io/initializers/) voor meer mogelijkheden\n",
    "\n",
    "* **Learning Function en Learning Rate** Het netwerk heeft nu als optimizer ADAM, het kan best zijn dat voor dit probleem er een betere oplossing is. [Hier](https://keras.io/optimizers/) wordt uitgelegd welke standaard optimizers er zijn, maar ook hoe een custom optimizer gemaakt kan worden. Maak een custom optimizer voor SGD met learning rate 0.1 en een passende learning rate decay erbij. Voeg dit toe aan je netwerk.\n",
    "\n",
    "* **Externe parameters** Ons netwerk runt nu maar een paar epochs, door dit te verhogen kan het netwerk beter convergeren. Pas echter op voor overfitting! Hier komen we in deel 2 op terug.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deel 2: Convolutional Neural Networks\n",
    "De stap om van een MLP naar een CNN te gaan is niet zo groot, in plaats van een geflattened image gebruiken we nu de originele images als input, en passen we ons model aan om convolutie en pooling layers toe te voegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Laad en normaliseer de data (opnieuw)\n",
    "train_X, train_y, val_X, val_y, test_X, test_y, label_to_names = load_data()\n",
    "\n",
    "# Conv nets trainen duurt erg lang op CPU, dus we gebruiken maar een klein deel\n",
    "# van de data nu, als er tijd over is kan je proberen je netwerk op de volledige set te runnen\n",
    "train_X = train_X[:10000]\n",
    "train_y = train_y[:10000]\n",
    "\n",
    "def calc_mean_std(X):\n",
    "    mean = np.mean(X)\n",
    "    std = np.std(X)\n",
    "    return mean, std\n",
    "\n",
    "def normalize(data, mean, std):\n",
    "    return (data-mean)/std\n",
    "\n",
    "#De data van train_X is genoeg om de mean en std van de hele set nauwkeurig te benaderen\n",
    "mean,std = calc_mean_std(train_X)\n",
    "test_X = normalize(test_X,mean,std)\n",
    "val_X = normalize(val_X,mean,std)\n",
    "train_X = normalize(train_X ,mean,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net():\n",
    "    # We definieren de input van het netwerk als de shape van de input,\n",
    "    # minus de dimensie van het aantal plaatjes, uiteindelijk dus (3, 32, 32).\n",
    "    input = Input(shape=train_X.shape[1:])\n",
    "    \n",
    "    # Eerste convolutielaag\n",
    "    # Padding valid betekent dat we enkel volledige convoluties gebruiken, zonder padding\n",
    "    # Data format channels_first betekent dat de channels eerst komen, en dan pas de size van ons plaatje\n",
    "    # Dus (3, 32, 32) in plaats van (32, 32, 3)\n",
    "    conv = Conv2D(filters=16, kernel_size=(3,3), padding='valid',\n",
    "                  data_format='channels_first', activation='relu')(input)\n",
    "    \n",
    "    # Nog een convolutielaag, dit keer met stride=2 om de inputsize te verkleinen\n",
    "    conv = Conv2D(filters=32, kernel_size=(3,3), padding='valid',\n",
    "                  data_format='channels_first', activation='relu', strides=(2, 2))(conv)\n",
    "    \n",
    "    #Voeg een flatten laag toe, om te schakelen naar de dense layer\n",
    "    flatten = Flatten()(conv)\n",
    "   \n",
    "    # De softmax laag voor de probabilities \n",
    "    output_layer = Dense(units=nr_classes, activation='softmax')(flatten)\n",
    "    \n",
    "    model = Model(inputs=input, outputs=output_layer)\n",
    "    \n",
    "    # Het model moet nog gecompiled worden en loss+learning functie gespecificeerd worden\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = conv_net()\n",
    "\n",
    "model.fit(x=train_X, y=train_y, batch_size=50, epochs=10, validation_data=(val_X, val_y), verbose=2)\n",
    "predictions = np.array(model.predict(test_X, batch_size=100))\n",
    "test_y = np.array(test_y, dtype=np.int32)\n",
    "#Pak de hoogste prediction\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "#Print resultaten\n",
    "print(\"Accuracy = {}\".format(np.sum(predictions == test_y) / float(len(predictions))))\n",
    "print(classification_report(test_y, predictions, target_names=list(label_to_names.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht\n",
    "Maak ook hier ons netwerk beter door bijvoorbeeld:\n",
    "\n",
    "* **Extra Layers**  Voeg net als bij de MLP extra layers toe, en experimenteer met de parameters. Kijk vooral ook naar dropout layers, aangezien het netwerk nu enorm aan het overfitten is kan dit een hele goede toevoeging zijn. Probeer ook pooling layers te gebruiken in plaats van een grotere stride voor downsampling van je netwerk. Idealiter wil je zoveel mogelijk lagen toevoegen en downsamplen tot je niet meer kan vanwege dimensie problemen.\n",
    "\n",
    "* **Meer filters of grotere filter size** Vaak beginnen netwerken met weinig filters en wordt het uitgebreid naar bijvoorbeeld 256 filters per convolutielaag. Dit zorgt wel voor langere runtimes. De filter size wordt tegenwoordig vaak op 3 gehouden, maar het kan in de eerste paar lagen effectief zijn om dit wat groter te maken, bijvoorbeeld 5 of 7.\n",
    "\n",
    "* **Andere parameters** Bedenk wat er nog meer verbeterd kan worden, en experimenteer hiermee!\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cap-scorer]",
   "language": "python",
   "name": "conda-env-cap-scorer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
